{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56691818",
   "metadata": {},
   "source": [
    "**Author: Guillermo Raya Garcia**<br>\n",
    "**NIU: 1568864**<br>\n",
    "**Universitat Autònoma de Barcelona**\n",
    "# Real Vs Fake Job Postings: An analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b572e4da",
   "metadata": {},
   "source": [
    "__[Link to our dataset](https://www.kaggle.com/shivamb/real-or-fake-fake-jobposting-prediction)__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e2a8a3",
   "metadata": {},
   "source": [
    "### 1. EDA (exploratory data analysis)\n",
    "**Preguntes:**\n",
    "* Quants atributs té la vostra base de dades?\n",
    "    * _**Resposta:** La nostra base de dades té 18 atributs._\n",
    "* Quin tipus d'atributs tens? (Númerics, temporals, categorics, binaris...)\n",
    "    * _**Resposta:** Els tipus d'atributs que tenim són els següents:_\n",
    "\n",
    "|| Attribute | Data type | Attribute description |\n",
    "|:--| :-: | :-: | :-- |\n",
    "|0|'job_id'| int | Row number |\n",
    "|1|'title'| str | Name of the job offered |\n",
    "|2|'location'| str | Location on which the job takes place, comprised of Country, State and City|\n",
    "|3|'department'| str | Department of the offered job |\n",
    "|4|'salary_range'| float| Salary range of the offered job |\n",
    "|5|'company_profile'| str | Brief overview of the company |\n",
    "|6|'description'| str | Description of the offered job |\n",
    "|7|'requirements'| str | Requirements for the applicants to the job |\n",
    "|8|'benefits'|float| Benefits included in the offer |\n",
    "|9|'telecommuting'|int| Availability |\n",
    "|10|'has_company_logo'|int| Existence and availability of a company logo |\n",
    "|11|'has_questions'|int| True if screening questions are present. |\n",
    "|12|'employment_type'|str| Type of employment (e.g.:'Full-time','Part-time', 'Minijob',etc.) |\n",
    "|13|'required_experience'|str| Required experience for applicants |\n",
    "|14|'required_education'|float| Required education for applicants |\n",
    "|15|'industry'|float| Industry of the job offered |\n",
    "|16|'function'|str| Function of the job offer |\n",
    "|17|'fraudulent'|int| Fraudulent: 1 if yes, otherwise 0 |\n",
    "\n",
    "* Com es el target, quantes categories diferents existeixen?\n",
    "    * _**Resposta:** El nostre target és `'fraudulent'`._\n",
    "    \n",
    "* Podeu veure alguna correlació entre X i y?\n",
    "    * _**Resposta:** Per intuició, podem assumir que, segurament, les ofertes de treball fraudulentes tindràn algunes característiques en comú (a les seves descripcions, per exemple), motiu pel qual podem respondre que probablement sí, hi hagi correlació entre `X` (variables explicatives) i `y`(variable objectiu)._\n",
    "    \n",
    "* Estan balancejades les etiquetes (distribució similar entre categories)? Creus que pot afectar a la classificació la seva distribució?\n",
    "    * _**Resposta:** No, no estan gens balancejades. Hi ha molt pocs casos fraudulents, i això pot esbiaixar la classificació (un classificador que digui \"no fraudulent\" per a qualsevol oferta de treball tindrà una bona accuracy, i això és inadmissible)._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b29ef7a",
   "metadata": {},
   "source": [
    "### 2. Preprocessing (normalitzation, outlier removal, feature selection...)\n",
    "**Preguntes:**\n",
    "* Estàn les dades normalitzades? Caldria fer-ho?\n",
    "    * _**Resposta:** No, les dades no estan normalitzades: alguns dels atributs que sospitem que seran més importants són str. Pensem que serà convenient normalitzar aquestes dades fent servir una vectorització TfIdf._\n",
    "* En cas que les normalitzeu, quin tipus de normalització será més adient per les vostres dades?\n",
    "    * _**Resposta:** Com hem esmentat a l'apartat anterior, aplicarem TfIdf als atributs de texte que creiem més rellevants (`'description'` i `company_profile`)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c40fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all, some imports:\n",
    "from sklearn.datasets import make_regression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.stats\n",
    "\n",
    "# Funcio per a llegir dades en format csv\n",
    "def load_dataset(path):\n",
    "    dataset = pd.read_csv(path, header=0, delimiter=',')\n",
    "    return dataset\n",
    "\n",
    "# Carreguem dataset\n",
    "dataset = load_dataset('fake_job_postings.csv')\n",
    "values = dataset.values\n",
    "labels = dataset.columns.values\n",
    "#values.shape[0] #Total number of rows: 17880\n",
    "#values.shape[1] #Total number of columns: 18\n",
    "\n",
    "#dataset #To previsualize the dataset\n",
    "#Auxiliary function to make cool charts.\n",
    "def chartMe(content,colnames,rownames,precision=9,pre=\"\", extra=\"\"):\n",
    "    #Making the rows of the chart\n",
    "    chart=[]\n",
    "    for i,valuesRow in enumerate(content):\n",
    "        newRow=[str(pre+\" \"+rownames[i]+\" \"+extra)]\n",
    "        newRow.extend([valuesRow])\n",
    "        chart.append(newRow)\n",
    "        \n",
    "    #Setting up the precision, as requested by function call\n",
    "    pd.set_option('precision', precision)\n",
    "    seriousChart=pd.DataFrame(chart, columns=colnames).style.hide_index()\n",
    "    pd.reset_option('precision')\n",
    "    \n",
    "    return seriousChart\n",
    "\n",
    "#Example: to chart the content of the first row of values\n",
    "#chartMe(content=values[1],colnames=[\"Attribute\",\"Value\"],rownames=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20aa2a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing:\n",
    "#Let's remove the first attribute from our data, since it only numbers our rows and gives us no useful information.\n",
    "dataset=dataset.drop(columns=('job_id'))\n",
    "\n",
    "#And since we will mainly be working with the description attribute, we want to remove all instances that have \"nan\" on it.\n",
    "dataset=dataset.dropna(axis=0,subset=['description'])\n",
    "dataset=dataset.dropna(axis=0,subset=['company_profile'])\n",
    "\n",
    "\n",
    "#Let's recalculate 'values' and 'labels' with the recently applied changes.\n",
    "values = dataset.values\n",
    "labels = dataset.columns.values\n",
    "\n",
    "#Let's set apart the X (input data, or explicative variables) from the y (target variable).\n",
    "X=values[:,0:16]\n",
    "y=values[:,-1:]\n",
    "\n",
    "#We'll split our data into a Training set and a Test set \n",
    "#    https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#\n",
    "#We will use K-fold cross-validation later down the road, this is just to get started.\n",
    "#    https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "#Since many important fields in this dataset are comprised solely of text, we will need to process them in some way so that we can compare them. And for that, we'll use scikit's tools for text feature extraction.\n",
    "\n",
    "#Taking a look at the documentation here:\n",
    "#    https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "#we choose to start by running a TfIdf vectorisation on the 'description' attributes, in order to try out our tools.\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = CountVectorizer(min_df=0.05, stop_words='english')\n",
    "tfidf_vectorizer= TfidfVectorizer(min_df=0.05, stop_words='english')\n",
    "X_t_desc = vectorizer.fit_transform(X_train[:,5])\n",
    "X_tfidf_desc = tfidf_vectorizer.fit_transform(X_train[:,5])\n",
    "#X_t_desc # This is the matrix that we get from count vectorizing the description attribute from X_train\n",
    "#X_tfidf_desc # This is the matrix that we get from vectorizing with TfIdf the description attribute from X_train\n",
    "#len(vectorizer.get_feature_names_out()) # This is the number of different words found in the countvectorisation.\n",
    "#tfidf_vectorizer.get_feature_names_out() # These are the words we found with TfIdf vectorisation. When we start analyzing other texts, we'll be looking for the presence and frequency of these words.\n",
    "#tfidf_vectorizer.vocabulary_.get('website') # ← This command gets us the position of a certain word in our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68519440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This next function will apply count vectorisation to a given string.\n",
    "# Input:\n",
    "    # · text: string wanted to be transformed using countvectorization.\n",
    "# Output:\n",
    "    # Matrix with the result of the countvectorization. Can be transformed into array (for readable results) using \".toarray()\"\n",
    "def vecCountTrans (text):\n",
    "    return(vectorizer.transform([text]))\n",
    "def vecCountTrans_array (text):\n",
    "    return(vectorizer.transform([text]).toarray())\n",
    "\n",
    "def vecTfidfTrans (text):\n",
    "    return(tfidf_vectorizer.transform([text]))\n",
    "def vecTfidfTrans_array (text):\n",
    "    return(tfidf_vectorizer.transform([text]).toarray())\n",
    "# ↓ And here's the result of vectorizing a new sentence of my choosing using the two vectorizing methods\n",
    "#vecCountTrans_array(X_train[0][5])\n",
    "#vecTfidfTrans_array(X_train[0][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4a8ed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This next function will apply vecCountTrans to our X_train and X_test sets.\n",
    "# Input:\n",
    "    # · myArray: Array containing a column that will be transformed using vecCountTrans\n",
    "    # · colNumber: The index (int) of the column that is desired to be transformed using vecCountTrans\n",
    "# Output:\n",
    "    # · newCol: Array containing the vectorisations of the elements in the chosen column of the given ndarray.\n",
    "def vecCountTrans_column(myArray,colNumber):\n",
    "    newCol=np.array([])\n",
    "    for row in range(myArray.shape[0]):\n",
    "        newCol=np.append(newCol,vecCountTrans(myArray[row,colNumber]))\n",
    "    return(newCol)\n",
    "\n",
    "# This other function will apply the previous function and append the resulting array onto the provided array, as a last column.\n",
    "# Input:\n",
    "    # · myArray: Array containing a column that will be transformed using vecCountTrans\n",
    "    # · colNumber: The index (int) of the column that is desired to be transformed using vecCountTrans\n",
    "# Output:\n",
    "    # · myNewArray: Array containing the same as myArray, but having the result of vecCountTrans as a last column\n",
    "def append_vecCountTrans_column(myArray,colNumber):\n",
    "    nC=vecCountTrans_column(myArray,colNumber)\n",
    "    nC=np.array([nC]).transpose()\n",
    "    myNewArray=np.append(myArray,nC,axis=1)\n",
    "    return(myNewArray)\n",
    "\n",
    "def vecCountTrans_niceArray(myArray,colNumber):\n",
    "    niceArray=np.array([])\n",
    "    for row in range(myArray.shape[0]):\n",
    "        newInstance=vecCountTrans_array(myArray[row,colNumber])[0]\n",
    "        niceArray=np.append(niceArray,newInstance)\n",
    "    return(niceArray.reshape((myArray.shape[0],len(vectorizer.get_feature_names_out()))))\n",
    "\n",
    "\n",
    "\n",
    "def vecTfidfTrans_column(myArray,colNumber):\n",
    "    newCol=np.array([])\n",
    "    for row in range(myArray.shape[0]):\n",
    "        newCol=np.append(newCol,vecTfidfTrans(myArray[row,colNumber]))\n",
    "    return(newCol)\n",
    "\n",
    "def append_vecTfidfTrans_column(myArray,colNumber):\n",
    "    nC=vecTfidfTrans_column(myArray,colNumber)\n",
    "    nC=np.array([nC]).transpose()\n",
    "    myNewArray=np.append(myArray,nC,axis=1)\n",
    "    return(myNewArray)\n",
    "\n",
    "def vecTfidfTrans_niceArray(myArray,colNumber):\n",
    "    niceArray=np.array([])\n",
    "    for row in range(myArray.shape[0]):\n",
    "        newInstance=vecTfidfTrans_array(myArray[row,colNumber])[0]\n",
    "        niceArray=np.append(niceArray,newInstance)\n",
    "    return(niceArray.reshape((myArray.shape[0],len(tfidf_vectorizer.get_feature_names_out()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6edfa1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vct_desc=vecTfidfTrans_niceArray(X_train,5)\n",
    "X_test_vct_desc=vecTfidfTrans_niceArray(X_test,5)\n",
    "\n",
    "X_train_vct_comProf=vecTfidfTrans_niceArray(X_train,4)\n",
    "X_test_vct_comProf=vecTfidfTrans_niceArray(X_test,4)\n",
    "\n",
    "X_train_vct_descAndComProf=np.concatenate((X_train_vct_desc,X_train_vct_comProf),axis=1)\n",
    "X_test_vct_descAndComProf=np.concatenate((X_test_vct_desc,X_test_vct_comProf),axis=1)\n",
    "\n",
    "flat_y_train=y_train.flatten().astype('int')\n",
    "flat_y_test=y_test.flatten().astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3623d6a",
   "metadata": {},
   "source": [
    "### 3. Model Selection\n",
    "**Preguntes:**\n",
    "* Quins models heu considerat?\n",
    "    * _**Resposta:** He escollit els models `logistic regression`, `SVM` (amb kernels `linear` i `rbf`), `perceptron`. Tots aquests mètodes els provaré amb les vectoritzacions dels atributs `description`, i de `company_profile`._\n",
    "* Quin creieu que serà el més precís?\n",
    "    * _**Resposta:** Suposo que el més precís serà el `perceptron`, amb un cost computacional més elevat._\n",
    "* Quin serà el més ràpid?\n",
    "    * _**Resposta:** Sospito que els més ràpids seran el `SVM` amb kernel tipus `linear` i el `logistic regression`, per la seva relativa simplicitat._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0d76783",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression model\n",
    "from sklearn.linear_model import LogisticRegression as LogReg\n",
    "LR = LogReg(random_state=0).fit(X_train_vct_descAndComProf, flat_y_train)\n",
    "LRscore_descAndComProf=LR.score(X_test_vct_descAndComProf,flat_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6a7e641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM model (linear kernel)\n",
    "from sklearn import svm\n",
    "svc_linear = svm.SVC(C=10.0, kernel='linear', probability=True).fit(X_train_vct_descAndComProf, flat_y_train)\n",
    "SVCscore_linear_descAndCompProf=svc_linear.score(X_test_vct_descAndComProf, flat_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c838f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM model (rbf kernel)\n",
    "from sklearn import svm\n",
    "svc_rbf = svm.SVC(C=10.0, kernel='rbf', gamma=0.9, probability=True).fit(X_train_vct_descAndComProf, flat_y_train)\n",
    "SVCscore_rbf_descAndCompProf=svc_rbf.score(X_test_vct_descAndComProf, flat_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96ae883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perceptron model\n",
    "from sklearn.linear_model import Perceptron\n",
    "perceptron = Perceptron(tol=1e-3)\n",
    "perceptron.fit(X_train_vct_descAndComProf, flat_y_train)\n",
    "PerceptronScore_descAndCompProf=perceptron.score(X_test_vct_descAndComProf, flat_y_test)\n",
    "#print (\"Percentage of correct classification of  perceptron model,  training with 66% of the data: \",PerceptronScore_descAndCompProf*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c5937cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chartMe(content,colnames,rownames,precision=9,pre=\"\", extra=\"\"):\n",
    "    #Making the rows of the chart\n",
    "    chart=[]\n",
    "    for i,valuesRow in enumerate(content):\n",
    "        newRow=[str(pre+\" \"+rownames[i]+\" \"+extra)]\n",
    "        try:\n",
    "            newRow.extend(valuesRow)\n",
    "        except:\n",
    "            newRow.append(valuesRow)\n",
    "        chart.append(newRow)\n",
    "        \n",
    "    #Setting up the precision, as requested by function call\n",
    "    pd.set_option('precision', precision)\n",
    "    seriousChart=pd.DataFrame(chart, columns=colnames).style.hide_index()\n",
    "    pd.reset_option('precision')\n",
    "    \n",
    "    return seriousChart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b02b457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_cce1c_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"col_heading level0 col0\" ></th>\n",
       "      <th class=\"col_heading level0 col1\" >Logistic Regression</th>\n",
       "      <th class=\"col_heading level0 col2\" >SVM (linear kernel)</th>\n",
       "      <th class=\"col_heading level0 col3\" >SVM (rbf kernel)</th>\n",
       "      <th class=\"col_heading level0 col4\" >Perceptron</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_cce1c_row0_col0\" class=\"data row0 col0\" > Precision </td>\n",
       "      <td id=\"T_cce1c_row0_col1\" class=\"data row0 col1\" >99.2098%</td>\n",
       "      <td id=\"T_cce1c_row0_col2\" class=\"data row0 col2\" >99.7713%</td>\n",
       "      <td id=\"T_cce1c_row0_col3\" class=\"data row0 col3\" >99.7505%</td>\n",
       "      <td id=\"T_cce1c_row0_col4\" class=\"data row0 col4\" >99.7089%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1fed34605c0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores=[LRscore_descAndComProf,SVCscore_linear_descAndCompProf,SVCscore_rbf_descAndCompProf,PerceptronScore_descAndCompProf]\n",
    "myContent=[[str(np.around(score*100,decimals=4))+\"%\" for score in scores]]\n",
    "myColnames=['','Logistic Regression','SVM (linear kernel)','SVM (rbf kernel)','Perceptron']\n",
    "myRownames=['Precision']\n",
    "scoresChart=chartMe(content=myContent,colnames=myColnames,rownames=myRownames)\n",
    "scoresChart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b80df4",
   "metadata": {},
   "source": [
    "### 4. Crossvalidation\n",
    "**Preguntes:**\n",
    "* Per què és important cross-validar els resultats?\n",
    "    * _**Resposta:** Per tal d'entrenar i posar a prova els models d'una forma robusta, que eviti resultats excessivament optimistes fruit de l'atzar._\n",
    "* Separa la base de dades en el conjunt de train-test. Com de fiables serán els resultats obtinguts? En quins casos serà més fiable, si tenim moltes dades d'entrenament o poques?\n",
    "    * _**Resposta:** Els resultats no seran tan fiables com fent una crossvalidació. Seràn més fiables quantes més dades d'entrenament tinguem._\n",
    "* Quin tipus de K-fold heu escollit? Quants conjunts heu seleccionat (quina k)?\n",
    "    * _**Resposta:** He escollit `k=7`, ja que he anat fent proves i el resultat m'ha semblat raonable._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0f25218",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crossvalidation: Precision and Recall scores\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score\n",
    "scoring = ['precision_macro', 'recall_macro']\n",
    "myK=7\n",
    "#LogReg\n",
    "cv_scores_lr = cross_validate(LR, X_train_vct_descAndComProf, flat_y_train, cv=myK, scoring=scoring)\n",
    "#SVM (linear)\n",
    "cv_scores_SVM_lin = cross_validate(svc_linear, X_train_vct_descAndComProf, flat_y_train, cv=myK, scoring=scoring)\n",
    "#SVM (rbf)\n",
    "cv_scores_SVM_rbf = cross_validate(svc_rbf, X_train_vct_descAndComProf, flat_y_train, cv=myK, scoring=scoring)\n",
    "#Perceptron\n",
    "cv_scores_perceptron = cross_validate(perceptron, X_train_vct_descAndComProf, flat_y_train, cv=myK, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3867d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_9a583_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th class=\"col_heading level0 col1\" >fit_time average</th>\n",
       "      <th class=\"col_heading level0 col2\" >score_time average</th>\n",
       "      <th class=\"col_heading level0 col3\" >test_precision_macro average</th>\n",
       "      <th class=\"col_heading level0 col4\" >test_precision_macro average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_9a583_row0_col0\" class=\"data row0 col0\" > Logistic Regression </td>\n",
       "      <td id=\"T_9a583_row0_col1\" class=\"data row0 col1\" >0.1555 s</td>\n",
       "      <td id=\"T_9a583_row0_col2\" class=\"data row0 col2\" >0.0045 s</td>\n",
       "      <td id=\"T_9a583_row0_col3\" class=\"data row0 col3\" >99.43%</td>\n",
       "      <td id=\"T_9a583_row0_col4\" class=\"data row0 col4\" >71.94%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9a583_row1_col0\" class=\"data row1 col0\" > SVM (linear kernel) </td>\n",
       "      <td id=\"T_9a583_row1_col1\" class=\"data row1 col1\" >6.2052 s</td>\n",
       "      <td id=\"T_9a583_row1_col2\" class=\"data row1 col2\" >0.0627 s</td>\n",
       "      <td id=\"T_9a583_row1_col3\" class=\"data row1 col3\" >94.93%</td>\n",
       "      <td id=\"T_9a583_row1_col4\" class=\"data row1 col4\" >95.55%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9a583_row2_col0\" class=\"data row2 col0\" > SVM (rbf kernel) </td>\n",
       "      <td id=\"T_9a583_row2_col1\" class=\"data row2 col1\" >38.6 s</td>\n",
       "      <td id=\"T_9a583_row2_col2\" class=\"data row2 col2\" >1.408 s</td>\n",
       "      <td id=\"T_9a583_row2_col3\" class=\"data row2 col3\" >99.79%</td>\n",
       "      <td id=\"T_9a583_row2_col4\" class=\"data row2 col4\" >89.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9a583_row3_col0\" class=\"data row3 col0\" > Perceptron </td>\n",
       "      <td id=\"T_9a583_row3_col1\" class=\"data row3 col1\" >0.1019 s</td>\n",
       "      <td id=\"T_9a583_row3_col2\" class=\"data row3 col2\" >0.0045 s</td>\n",
       "      <td id=\"T_9a583_row3_col3\" class=\"data row3 col3\" >95.55%</td>\n",
       "      <td id=\"T_9a583_row3_col4\" class=\"data row3 col4\" >93.79%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1fed69d89b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores_names=[cv_scores_lr,cv_scores_SVM_lin,cv_scores_SVM_rbf,cv_scores_perceptron]\n",
    "seriousContent=[]\n",
    "for name in scores_names:\n",
    "    seriousRow=[]\n",
    "    for e,i in zip(name,range(len(name))):\n",
    "        if(i<2):\n",
    "            seriousRow.append(str(np.around(np.average(name[e]),decimals=4))+' s')\n",
    "        else:\n",
    "            seriousRow.append(str(np.around(np.average(name[e])*100,decimals=2))+'%')    \n",
    "    seriousContent.append(seriousRow)\n",
    "seriousColnames=['Model','fit_time average','score_time average','test_precision_macro average','test_precision_macro average']\n",
    "seriousRownames=['Logistic Regression','SVM (linear kernel)','SVM (rbf kernel)','Perceptron']\n",
    "seriousChart=chartMe(content=seriousContent,colnames=seriousColnames,rownames=seriousRownames)\n",
    "display(seriousChart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d34476",
   "metadata": {},
   "source": [
    "### 5. Anàlisi de resultats:\n",
    "\n",
    "Els models més lents tant en entrenament com a classificació han estat els de SVM, it tot i que el SVM lineal ha donat molts bons resultats de precision-recall, el model de perceptron (seixanta vegades més ràpid d'entrenar i 15 vegades més ràpid en classificació) ha donat resultats d'una qualitat comparable, amb una precision del 95.55% i una recall del 93.78%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2ef739",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
