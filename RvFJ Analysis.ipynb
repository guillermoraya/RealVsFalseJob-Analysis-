{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56691818",
   "metadata": {},
   "source": [
    "**Author: Guillermo Raya Garcia**<br>\n",
    "**NIU: 1568864**<br>\n",
    "**Universitat Autònoma de Barcelona**\n",
    "# Real Vs Fake Job Postings: An analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b572e4da",
   "metadata": {},
   "source": [
    "__[Link to our dataset](https://www.kaggle.com/shivamb/real-or-fake-fake-jobposting-prediction)__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e2a8a3",
   "metadata": {},
   "source": [
    "### 1. EDA (exploratory data analysis)\n",
    "**Preguntes:**\n",
    "* Quants atributs té la vostra base de dades?\n",
    "    * _**Resposta:** La nostra base de dades té 18 atributs._\n",
    "* Quin tipus d'atributs tens? (Númerics, temporals, categorics, binaris...)\n",
    "    * _**Resposta:** Els tipus d'atributs que tenim són els següents:_\n",
    "\n",
    "|| Attribute | Data type | Attribute description |\n",
    "|:--| :-: | :-: | :-- |\n",
    "|0|'job_id'| int | Row number |\n",
    "|1|'title'| str | Name of the job offered |\n",
    "|2|'location'| str | Location on which the job takes place, comprised of Country, State and City|\n",
    "|3|'department'| str | Department of the offered job |\n",
    "|4|'salary_range'| float| Salary range of the offered job |\n",
    "|5|'company_profile'| str | Brief overview of the company |\n",
    "|6|'description'| str | Description of the offered job |\n",
    "|7|'requirements'| str | Requirements for the applicants to the job |\n",
    "|8|'benefits'|float| Benefits included in the offer |\n",
    "|9|'telecommuting'|int| Availability |\n",
    "|10|'has_company_logo'|int| Existence and availability of a company logo |\n",
    "|11|'has_questions'|int| TODO |\n",
    "|12|'employment_type'|str| Type of employment (e.g.:'Full-time','Part-time', 'Minijob',etc.) |\n",
    "|13|'required_experience'|str| Required experience for applicants |\n",
    "|14|'required_education'|float| Required education for applicants |\n",
    "|15|'industry'|float| Industry of the job offered |\n",
    "|16|'function'|str| Function of the job offer |\n",
    "|17|'fraudulent'|int| Fraudulent: 1 if yes, otherwise 0 |\n",
    "\n",
    "* Com es el target, quantes categories diferents existeixen?\n",
    "    * _**Resposta:** El nostre target és `'fraudulent'`._\n",
    "    \n",
    "* Podeu veure alguna correlació entre X i y?\n",
    "    * _**Resposta:** Per intuició, podem assumir que, segurament, les ofertes de treball fraudulentes tindràn algunes característiques en comú (a les seves descripcions, per exemple), motiu pel qual podem respondre que probablement sí, hi hagi correlació entre `X` (variables explicatives) i `y`(variable objectiu)._\n",
    "    \n",
    "* Estan balancejades les etiquetes (distribució similar entre categories)? Creus que pot afectar a la classificació la seva distribució?\n",
    "    * _**Resposta:** No, no estan gens balancejades. Hi ha molt pocs casos fraudulents, i això pot esbiaixar la classificació (un classificador que digui \"no fraudulent\" per a qualsevol oferta de treball tindrà una bona accuracy, i això és inadmissible)._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b29ef7a",
   "metadata": {},
   "source": [
    "### 2. Preprocessing (normalitzation, outlier removal, feature selection...)\n",
    "**Preguntes:**\n",
    "* Estàn les dades normalitzades? Caldria fer-ho?\n",
    "    * _**Resposta:** No, les dades no estan normalitzades: alguns dels atributs que sospitem que seran més importants són str. Pensem que serà convenient normalitzar aquestes dades fent servir una vectorització TfIdf._\n",
    "* En cas que les normalitzeu, quin tipus de normalització será més adient per les vostres dades?\n",
    "    * _**Resposta:** Com hem esmentat a l'apartat anterior, aplicarem TfIdf als atributs de texte que creiem més rellevants (`'description'` i `company_profile`)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c40fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all, some imports:\n",
    "from sklearn.datasets import make_regression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.stats\n",
    "\n",
    "# Funcio per a llegir dades en format csv\n",
    "def load_dataset(path):\n",
    "    dataset = pd.read_csv(path, header=0, delimiter=',')\n",
    "    return dataset\n",
    "\n",
    "# Carreguem dataset\n",
    "dataset = load_dataset('fake_job_postings.csv')\n",
    "values = dataset.values\n",
    "labels = dataset.columns.values\n",
    "#values.shape[0] #Total number of rows: 17880\n",
    "#values.shape[1] #Total number of columns: 18\n",
    "\n",
    "#dataset #To previsualize the dataset\n",
    "#Auxiliary function to make cool charts.\n",
    "def chartMe(content,colnames,rownames,precision=9,pre=\"\", extra=\"\"):\n",
    "    #Making the rows of the chart\n",
    "    chart=[]\n",
    "    for i,valuesRow in enumerate(content):\n",
    "        newRow=[str(pre+\" \"+rownames[i]+\" \"+extra)]\n",
    "        newRow.extend([valuesRow])\n",
    "        chart.append(newRow)\n",
    "        \n",
    "    #Setting up the precision, as requested by function call\n",
    "    pd.set_option('precision', precision)\n",
    "    seriousChart=pd.DataFrame(chart, columns=colnames).style.hide_index()\n",
    "    pd.reset_option('precision')\n",
    "    \n",
    "    return seriousChart\n",
    "\n",
    "#Example: to chart the content of the first row of values\n",
    "#chartMe(content=values[1],colnames=[\"Attribute\",\"Value\"],rownames=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20aa2a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing:\n",
    "#Let's remove the first attribute from our data, since it only numbers our rows and gives us no useful information.\n",
    "dataset=dataset.drop(columns=('job_id'))\n",
    "\n",
    "#And since we will mainly be working with the description attribute, we want to remove all instances that have \"nan\" on it.\n",
    "dataset=dataset.dropna(axis=0,subset=['description'])\n",
    "dataset=dataset.dropna(axis=0,subset=['company_profile'])\n",
    "\n",
    "\n",
    "#Let's recalculate 'values' and 'labels' with the recently applied changes.\n",
    "values = dataset.values\n",
    "labels = dataset.columns.values\n",
    "\n",
    "#Let's set apart the X (input data, or explicative variables) from the y (target variable).\n",
    "X=values[:,0:16]\n",
    "y=values[:,-1:]\n",
    "\n",
    "#We'll split our data into a Training set and a Test set \n",
    "#    https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#\n",
    "#We will use K-fold cross-validation later down the road, this is just to get started.\n",
    "#    https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "#Since many important fields in this dataset are comprised solely of text, we will need to process them in some way so that we can compare them. And for that, we'll use scikit's tools for text feature extraction.\n",
    "\n",
    "#Taking a look at the documentation here:\n",
    "#    https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "#we choose to start by running a TfIdf vectorisation on the 'description' attributes, in order to try out our tools.\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = CountVectorizer(min_df=0.05, stop_words='english')\n",
    "tfidf_vectorizer= TfidfVectorizer(min_df=0.05, stop_words='english')\n",
    "X_t_desc = vectorizer.fit_transform(X_train[:,5])\n",
    "X_tfidf_desc = tfidf_vectorizer.fit_transform(X_train[:,5])\n",
    "#X_t_desc # This is the matrix that we get from count vectorizing the description attribute from X_train\n",
    "#X_tfidf_desc # This is the matrix that we get from vectorizing with TfIdf the description attribute from X_train\n",
    "#len(vectorizer.get_feature_names_out()) # This is the number of different words found in the countvectorisation.\n",
    "#tfidf_vectorizer.get_feature_names_out() # These are the words we found with TfIdf vectorisation. When we start analyzing other texts, we'll be looking for the presence and frequency of these words.\n",
    "#tfidf_vectorizer.vocabulary_.get('website') # ← This command gets us the position of a certain word in our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68519440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This next function will apply count vectorisation to a given string.\n",
    "# Input:\n",
    "    # · text: string wanted to be transformed using countvectorization.\n",
    "# Output:\n",
    "    # Matrix with the result of the countvectorization. Can be transformed into array (for readable results) using \".toarray()\"\n",
    "def vecCountTrans (text):\n",
    "    return(vectorizer.transform([text]))\n",
    "def vecCountTrans_array (text):\n",
    "    return(vectorizer.transform([text]).toarray())\n",
    "\n",
    "def vecTfidfTrans (text):\n",
    "    return(tfidf_vectorizer.transform([text]))\n",
    "def vecTfidfTrans_array (text):\n",
    "    return(tfidf_vectorizer.transform([text]).toarray())\n",
    "# ↓ And here's the result of vectorizing a new sentence of my choosing using the two vectorizing methods\n",
    "#vecCountTrans_array(X_train[0][5])\n",
    "#vecTfidfTrans_array(X_train[0][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4a8ed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This next function will apply vecCountTrans to our X_train and X_test sets.\n",
    "# Input:\n",
    "    # · myArray: Array containing a column that will be transformed using vecCountTrans\n",
    "    # · colNumber: The index (int) of the column that is desired to be transformed using vecCountTrans\n",
    "# Output:\n",
    "    # · newCol: Array containing the vectorisations of the elements in the chosen column of the given ndarray.\n",
    "def vecCountTrans_column(myArray,colNumber):\n",
    "    newCol=np.array([])\n",
    "    for row in range(myArray.shape[0]):\n",
    "        newCol=np.append(newCol,vecCountTrans(myArray[row,colNumber]))\n",
    "    return(newCol)\n",
    "\n",
    "# This other function will apply the previous function and append the resulting array onto the provided array, as a last column.\n",
    "# Input:\n",
    "    # · myArray: Array containing a column that will be transformed using vecCountTrans\n",
    "    # · colNumber: The index (int) of the column that is desired to be transformed using vecCountTrans\n",
    "# Output:\n",
    "    # · myNewArray: Array containing the same as myArray, but having the result of vecCountTrans as a last column\n",
    "def append_vecCountTrans_column(myArray,colNumber):\n",
    "    nC=vecCountTrans_column(myArray,colNumber)\n",
    "    nC=np.array([nC]).transpose()\n",
    "    myNewArray=np.append(myArray,nC,axis=1)\n",
    "    return(myNewArray)\n",
    "\n",
    "def vecCountTrans_niceArray(myArray,colNumber):\n",
    "    niceArray=np.array([])\n",
    "    for row in range(myArray.shape[0]):\n",
    "        newInstance=vecCountTrans_array(myArray[row,colNumber])[0]\n",
    "        niceArray=np.append(niceArray,newInstance)\n",
    "    return(niceArray.reshape((myArray.shape[0],len(vectorizer.get_feature_names_out()))))\n",
    "\n",
    "\n",
    "\n",
    "def vecTfidfTrans_column(myArray,colNumber):\n",
    "    newCol=np.array([])\n",
    "    for row in range(myArray.shape[0]):\n",
    "        newCol=np.append(newCol,vecTfidfTrans(myArray[row,colNumber]))\n",
    "    return(newCol)\n",
    "\n",
    "def append_vecTfidfTrans_column(myArray,colNumber):\n",
    "    nC=vecTfidfTrans_column(myArray,colNumber)\n",
    "    nC=np.array([nC]).transpose()\n",
    "    myNewArray=np.append(myArray,nC,axis=1)\n",
    "    return(myNewArray)\n",
    "\n",
    "def vecTfidfTrans_niceArray(myArray,colNumber):\n",
    "    niceArray=np.array([])\n",
    "    for row in range(myArray.shape[0]):\n",
    "        newInstance=vecTfidfTrans_array(myArray[row,colNumber])[0]\n",
    "        niceArray=np.append(niceArray,newInstance)\n",
    "    return(niceArray.reshape((myArray.shape[0],len(tfidf_vectorizer.get_feature_names_out()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6edfa1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vct_desc=vecTfidfTrans_niceArray(X_train,5)\n",
    "X_test_vct_desc=vecTfidfTrans_niceArray(X_test,5)\n",
    "\n",
    "X_train_vct_comProf=vecTfidfTrans_niceArray(X_train,4)\n",
    "X_test_vct_comProf=vecTfidfTrans_niceArray(X_test,4)\n",
    "\n",
    "X_train_vct_descAndComProf=np.concatenate((X_train_vct_desc,X_train_vct_comProf),axis=1)\n",
    "X_test_vct_descAndComProf=np.concatenate((X_test_vct_desc,X_test_vct_comProf),axis=1)\n",
    "\n",
    "flat_y_test=y_test.flatten().astype('int')\n",
    "flat_y_train=y_train.flatten().astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8401af",
   "metadata": {},
   "source": [
    "### 3. Model Selection\n",
    "**Preguntes:**\n",
    "* Quins models heu considerat?\n",
    "    * _**Resposta:** He escollit els models `logistic regression`, `SVM` (amb kernels `linear` i `rbf`) i `perceptron`. Tots aquests mètodes els provaré amb les vectoritzacions dels atributs `description`, i de `company_profile`._\n",
    "* Quin creieu que serà el més precís?\n",
    "    * _**Resposta:** Suposo que el més precís serà el `perceptron`, amb un cost computacional més elevat._\n",
    "* Quin serà el més ràpid?\n",
    "    * _**Resposta:** Sospito que els més ràpids seran el `SVM` amb kernel tipus `linear` i el `logistic regression`, per la seva relativa simplicitat._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f2c8ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of correct classification of  Logistic model,  training with 66% of the data:  99.20981493033895 %\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression model\n",
    "from sklearn.linear_model import LogisticRegression as LogReg\n",
    "clf = LogReg(random_state=0).fit(X_train_vct_descAndComProf, flat_y_train)\n",
    "#LRscore_descAndComProf=clf.score(X_test_vct_descAndComProf,flat_y_test)\n",
    "#print (\"Percentage of correct classification of  Logistic model,  training with 66% of the data: \",LRscore_descAndComProf*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "792b4d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of correct classification of  SVM model (linear kernel),  training with 66% of the data:  99.77126221667706 %\n"
     ]
    }
   ],
   "source": [
    "#SVM model (linear kernel)\n",
    "from sklearn import svm\n",
    "svc = svm.SVC(C=10.0, kernel='linear', probability=True).fit(X_train_vct_descAndComProf, flat_y_train)\n",
    "#SVCscore_linear_descAndCompProf=svc.score(X_test_vct_descAndComProf, flat_y_test)\n",
    "#print (\"Percentage of correct classification of  SVM model (linear kernel),  training with 66% of the data: \",SVCscore_linear_descAndCompProf*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "145e83cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of correct classification of  SVM model (rbf kernel),  training with 66% of the data:  99.75046787273861 %\n"
     ]
    }
   ],
   "source": [
    "#SVM model (rbf kernel)\n",
    "from sklearn import svm\n",
    "svc = svm.SVC(C=10.0, kernel='rbf', gamma=0.9, probability=True).fit(X_train_vct_descAndComProf, flat_y_train)\n",
    "#SVCscore_descAndCompProf=svc.score(X_test_vct_descAndComProf, flat_y_test)\n",
    "#print (\"Percentage of correct classification of  SVM model (rbf kernel),  training with 66% of the data: \",SVCscore_descAndCompProf*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "884ceed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of correct classification of  perceptron model,  training with 66% of the data:  99.70887918486171 %\n"
     ]
    }
   ],
   "source": [
    "#Perceptron model\n",
    "from sklearn.linear_model import Perceptron\n",
    "perceptron = Perceptron(tol=1e-3)\n",
    "perceptron.fit(X_train_vct_descAndComProf, flat_y_train)\n",
    "PerceptronScore_descAndCompProf=perceptron.score(X_test_vct_descAndComProf, flat_y_test)\n",
    "print (\"Percentage of correct classification of  perceptron model,  training with 66% of the data: \",PerceptronScore_descAndCompProf*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2da69ad3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 341 features, but SVC is expecting 682 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10316/1880924977.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision_recall_curve\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage_precision_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_vct_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    832\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    833\u001b[0m         \"\"\"\n\u001b[1;32m--> 834\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    835\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobA_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobB_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    836\u001b[0m             raise NotFittedError(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    596\u001b[0m                 \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"C\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m                 \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m                 \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m             )\n\u001b[0;32m    600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ensure_2d\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    394\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m             raise ValueError(\n\u001b[1;32m--> 396\u001b[1;33m                 \u001b[1;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    397\u001b[0m                 \u001b[1;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m             )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 341 features, but SVC is expecting 682 features as input."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_recall_curve, average_precision_score, roc_curve, auc\n",
    "\n",
    "probs = svc.predict_proba(X_test_vct_desc)\n",
    "n_classes=2\n",
    "\n",
    "# Compute Precision-Recall and plot curve for SVC model\n",
    "precision = {}\n",
    "recall = {}\n",
    "average_precision = {}\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(flat_y_test == i, probs[:, i])\n",
    "    average_precision[i] = average_precision_score(flat_y_test == i, probs[:, i])\n",
    "\n",
    "    plt.plot(recall[i], precision[i],\n",
    "    label='Precision-recall curve of class {0} (area = {1:0.2f})'\n",
    "                           ''.format(i, average_precision[i]))\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "\n",
    "    \n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "roc_auc = {}\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(flat_y_test == i, probs[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(15,10))\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})' ''.format(i, roc_auc[i]))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cb0683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_recall_curve, average_precision_score, roc_curve, auc\n",
    "\n",
    "probs = svc.predict_proba(X_test_vct_descAndComProf)\n",
    "n_classes=2\n",
    "\n",
    "# Compute Precision-Recall and plot curve for SVC model\n",
    "precision = {}\n",
    "recall = {}\n",
    "average_precision = {}\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(flat_y_test == i, probs[:, i])\n",
    "    average_precision[i] = average_precision_score(flat_y_test == i, probs[:, i])\n",
    "\n",
    "    plt.plot(recall[i], precision[i],\n",
    "    label='Precision-recall curve of class {0} (area = {1:0.2f})'\n",
    "                           ''.format(i, average_precision[i]))\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "\n",
    "    \n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "roc_auc = {}\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(flat_y_test == i, probs[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(15,10))\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})' ''.format(i, roc_auc[i]))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92e6f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
